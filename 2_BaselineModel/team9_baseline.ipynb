{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen und Pipeline definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Daten einlesen und zusammenführen\n",
    "df1 = pd.read_csv('../umsatzdaten_gekuerzt.csv')\n",
    "df2 = pd.read_csv('../wetter.csv')\n",
    "df3 = pd.read_csv('../kiwo.csv')\n",
    "df4 = pd.read_csv('../Feiertage_Brueckentage_Ferien_final.csv')\n",
    "\n",
    "df = df1.merge(df2, on='Datum', how='left')\n",
    "df = df.merge(df3, on='Datum', how='left')\n",
    "df = df.merge(df4, on='Datum', how='left')\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df['KielerWoche'] = df['KielerWoche'].fillna(False)\n",
    "df['KielerWoche'] = df['KielerWoche'].astype('bool')\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# 2. Pipeline-Komponenten definieren\n",
    "\n",
    "# a) Feature Engineering: Erstellen von Datum- und zyklischen Features\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # Basismerkmale\n",
    "    df['Jahr'] = df['Datum'].dt.year\n",
    "    df['Monat'] = df['Datum'].dt.month\n",
    "    df['Wochentag'] = df['Datum'].dt.weekday\n",
    "    df['Kalenderwoche'] = df['Datum'].dt.isocalendar().week\n",
    "    df['Tag_im_Jahr'] = df['Datum'].dt.dayofyear\n",
    "    df['Ist_Wochenende'] = df['Wochentag'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Zyklische Merkmale\n",
    "    df['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Monat_sin'] = np.sin(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Monat_cos'] = np.cos(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Wochentag_sin'] = np.sin(2 * np.pi * df['Wochentag'] / 7)\n",
    "    df['Wochentag_cos'] = np.cos(2 * np.pi * df['Wochentag'] / 7)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Entfernen der 'Datum'-Spalte nach der Feature-Erstellung\n",
    "    df = df.drop('Datum', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# b) FunctionTransformer für Feature Engineering\n",
    "feature_engineering = FunctionTransformer(add_features, validate=False)\n",
    "\n",
    "# c) Definieren der Feature-Gruppen\n",
    "numeric_features = ['Temperatur', 'Bewoelkung', 'Windgeschwindigkeit',\n",
    "                    'Tag_im_Jahr_sin', 'Tag_im_Jahr_cos',\n",
    "                    'Monat_sin', 'Monat_cos',\n",
    "                    'Wochentag_sin', 'Wochentag_cos','feiertag', 'KielerWoche',\n",
    "                    'brueckentag', 'BW', 'BY', 'B', 'BB', 'HB', 'HH', 'HE', 'MV',\n",
    "                      'NI', 'NW', 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
    "                    \n",
    "categorical_features = ['Warengruppe', 'Wettercode', 'Wochentag']\n",
    "\n",
    "# d) Definieren des ColumnTransformers\n",
    "# Stellen Sie sicher, dass 'Wettercode' als String vorliegt\n",
    "df['Wettercode'] = df['Wettercode'].astype(str)\n",
    "\n",
    "# Liste der Kategorien für 'Wettercode'\n",
    "wettercode_categories = [str(i) for i in range(0, 100)] + ['Unbekannt']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        ('wettercode', OneHotEncoder(categories=[wettercode_categories], handle_unknown='ignore'), ['Wettercode']),\n",
    "        \n",
    "        ('warengruppe', OneHotEncoder(handle_unknown='ignore'), ['Warengruppe']),\n",
    "        \n",
    "        ('wochentag', OneHotEncoder(handle_unknown='ignore'), ['Wochentag'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Define your date thresholds\n",
    "train_end_date = '2017-07-31'\n",
    "validation_end_date = '2018-08-31'\n",
    "\n",
    "train_data = df[df['Datum'] <= train_end_date]\n",
    "test_data = df[df['Datum'] > train_end_date]\n",
    "\n",
    "X = df.drop(['Umsatz'], axis=1)\n",
    "y = df['Umsatz']\n",
    "\n",
    "X_train  = train_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_train = train_data['Umsatz']\n",
    "\n",
    "X_test  = test_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_test = test_data['Umsatz']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Zielvariable und Features\n",
    "X = df.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y = df['Umsatz']\n",
    "\n",
    "# Split in Trainings- und Testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² on test: 0.7845507262960318\n",
      "adjusted R² on test: 0.7839720593597835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipeline_1 = Pipeline(steps=[\n",
    "    ('feature_engineering', feature_engineering),\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model',  LinearRegression())\n",
    "])\n",
    "pipeline_1.fit(X_train, y_train)\n",
    "\n",
    "lr_y_pred = pipeline_1.predict(X_test)\n",
    "\n",
    "def r2(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" R2 Score \"\"\"\n",
    "    return r2_score(actual, predicted)\n",
    "def adjr2(actual: np.ndarray, predicted: np.ndarray, rowcount: int, featurecount: int):\n",
    "    \"\"\" R2 Score \"\"\"\n",
    "    return 1-(1-r2(actual,predicted))*(rowcount-1)/(rowcount-featurecount)\n",
    "\n",
    "\n",
    "\n",
    "print(\"R² on test:\", r2_score(y_test, lr_y_pred))\n",
    "print(\"adjusted R² on test:\", adjr2(y_test, lr_y_pred, len(df), df.drop(['Umsatz'], axis=1) .shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² on test: 0.9195428572960962\n",
      "adjusted R² on test: 0.7839720593597835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', feature_engineering),\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Modell trainieren\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 5. Vorhersagen treffen und evaluieren\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"R² on test:\", r2_score(y_test, y_pred))\n",
    "print(\"adjusted R² on test:\", adjr2(y_test, lr_y_pred, len(y), X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Hyperparameter-Tuning (optional)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 500],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF Params:\", grid_search.best_params_)\n",
    "y_pred_best = grid_search.predict(X_test)\n",
    "print(\"Test R² Best RF:\", r2_score(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(y_test, y_pred_best, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonale Linie\n",
    "plt.xlabel(\"Tatsächliche Werte\")\n",
    "plt.ylabel(\"Vorhergesagte Werte\")\n",
    "plt.title(\"Tatsächliche Werte vs. Vorhergesagte Werte\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "residuals = y_test - y_pred_best\n",
    "\n",
    "# Residualplot\n",
    "plt.scatter(y_pred_best, residuals, alpha=0.5, color='g')\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Vorhergesagte Werte\")\n",
    "plt.ylabel(\"Residuen\")\n",
    "plt.title(\"Residualplot\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogramm der Residuen\n",
    "sns.histplot(residuals, kde=True, color='y')\n",
    "plt.axvline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Residuen\")\n",
    "plt.title(\"Verteilung der Residuen\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(y_test.values, label=\"Tatsächliche Werte\")\n",
    "plt.plot(y_pred_best, label=\"Vorhergesagte Werte\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Umsatz\")\n",
    "plt.title(\"Vergleich der tatsächlichen und vorhergesagten Werte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell auf Submission-Datensatz laufen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission vorbereiten\n",
    "\n",
    "\n",
    "df_sub = pd.read_csv('../sample_submission.csv')\n",
    "\n",
    "# Angenommen, die Jahre sind 20xx:\n",
    "df_sub['id'] = df_sub['id'].astype('string')\n",
    "df_sub['Tag'] = df_sub['id'].str[0:2]\n",
    "df_sub['Monat'] = df_sub['id'].str[2:4]\n",
    "df_sub['Jahr'] = '20' + df_sub['id'].str[4:6]  # falls im Format YY z. B. 01 → 2001 oder 20xx\n",
    "\n",
    "df_sub['Warengruppe'] = df_sub['id'].str[6]  # letztes Zeichen\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype(int)\n",
    "\n",
    "# Ein Datum aus Tag, Monat, Jahr erzeugen:\n",
    "df_sub.rename(columns={'Jahr':'year', 'Monat':'month', 'Tag':'day'}, inplace=True)\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub[['year','month','day']])\n",
    "df2['Datum'] = pd.to_datetime(df2['Datum'])\n",
    "df3['Datum'] = pd.to_datetime(df3['Datum'])\n",
    "df_sub = df_sub.merge(df3, on='Datum', how='left')\n",
    "df_sub = df_sub.merge(df2, on='Datum', how='left')\n",
    "\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5,6])\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat']/12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat']/12)\n",
    "\n",
    "# Check if all data types are correctly set for all variables\n",
    "print('### Initial datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "# Set the correct types for all variables\n",
    "\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].astype('category')\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype('category')\n",
    "\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].fillna(False)\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].astype('bool')\n",
    "print('### Corrected datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "\n",
    "# Datum in Datetime konvertieren\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub['Datum'])\n",
    "\n",
    "# Basismerkmale\n",
    "df_sub['Jahr'] = df_sub['Datum'].dt.year\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Kalenderwoche'] = df_sub['Datum'].dt.isocalendar().week\n",
    "df_sub['Tag_im_Jahr'] = df_sub['Datum'].dt.dayofyear\n",
    "df_sub['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "# Zyklische Merkmale\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Wochentag_sin'] = np.sin(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "df_sub['Wochentag_cos'] = np.cos(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "\n",
    "\n",
    "# Beispiel für Mittelwert-Imputation\n",
    "df_sub['Temperatur'].fillna(df_sub['Temperatur'].mean(), inplace=True)\n",
    "df_sub['Bewoelkung'].fillna(df_sub['Bewoelkung'].mean(), inplace=True)\n",
    "df_sub['Windgeschwindigkeit'].fillna(df_sub['Windgeschwindigkeit'].mean(), inplace=True)\n",
    "\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].cat.add_categories(['Unbekannt'])\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].fillna('Unbekannt')\n",
    "\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Vorhersagen treffen und evaluieren\n",
    "y_pred_sub = pipeline.predict(df_sub)\n",
    "y_pred_sub_best = grid_search.predict(df_sub)\n",
    "\n",
    "df_sub['Umsatz'] = y_pred_sub_best\n",
    "\n",
    "# Erstellen der finalen Submission:\n",
    "df_sub[['id','Umsatz']].to_csv('../submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
