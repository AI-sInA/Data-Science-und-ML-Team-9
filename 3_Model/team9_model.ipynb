{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen und Pipeline definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "def r2(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" R2 Score \"\"\"\n",
    "    return r2_score(actual, predicted)\n",
    "def adjr2(actual: np.ndarray, predicted: np.ndarray, rowcount: int, featurecount: int):\n",
    "    \"\"\" R2 Score \"\"\"\n",
    "    return 1-(1-r2(actual,predicted))*(rowcount-1)/(rowcount-featurecount)\n",
    "\n",
    "# 1. Daten einlesen und zusammenführen\n",
    "df1 = pd.read_csv('../umsatzdaten_gekuerzt.csv')\n",
    "df2 = pd.read_csv('../wetter.csv')\n",
    "df3 = pd.read_csv('../kiwo.csv')\n",
    "df4 = pd.read_csv('../Feier_Bruecke_Ferien_bis2018.csv')\n",
    "\n",
    "df = df1.merge(df2, on='Datum', how='left')\n",
    "df = df.merge(df3, on='Datum', how='left')\n",
    "df = df.merge(df4, on='Datum', how='left')\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df['KielerWoche'] = df['KielerWoche'].fillna(False)\n",
    "df['KielerWoche'] = df['KielerWoche'].astype('bool')\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# 2. Pipeline-Komponenten definieren\n",
    "\n",
    "# a) Feature Engineering: Erstellen von Datum- und zyklischen Features\n",
    "def add_features(df):\n",
    "    # df = df.copy()\n",
    "    # Basismerkmale\n",
    "    df['Jahr'] = df['Datum'].dt.year\n",
    "    df['Monat'] = df['Datum'].dt.month\n",
    "    df['Wochentag'] = df['Datum'].dt.weekday\n",
    "    df['Kalenderwoche'] = df['Datum'].dt.isocalendar().week\n",
    "    df['Tag_im_Jahr'] = df['Datum'].dt.dayofyear\n",
    "    df['Ist_Wochenende'] = df['Wochentag'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Zyklische Merkmale\n",
    "    df['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Monat_sin'] = np.sin(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Monat_cos'] = np.cos(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Wochentag_sin'] = np.sin(2 * np.pi * df['Wochentag'] / 7)\n",
    "    df['Wochentag_cos'] = np.cos(2 * np.pi * df['Wochentag'] / 7)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = add_features(df)\n",
    "# b) FunctionTransformer für Feature Engineering\n",
    "feature_engineering = FunctionTransformer(add_features, validate=False)\n",
    "\n",
    "# c) Definieren der Feature-Gruppen\n",
    "numeric_features = ['Temperatur', 'Bewoelkung', 'Windgeschwindigkeit',\n",
    "                    'Tag_im_Jahr_sin', 'Tag_im_Jahr_cos',\n",
    "                    'Monat_sin', 'Monat_cos',\n",
    "                    'Wochentag_sin', 'Wochentag_cos','feiertag', 'KielerWoche',\n",
    "                    'brueckentag', 'BW', 'BY', 'B', 'BB', 'HB', 'HH', 'HE', 'MV',\n",
    "                      'NI', 'NW', 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
    "                    \n",
    "categorical_features = ['Warengruppe', 'Wettercode', 'Wochentag']\n",
    "\n",
    "# d) Definieren des ColumnTransformers\n",
    "# Stellen Sie sicher, dass 'Wettercode' als String vorliegt\n",
    "df['Wettercode'] = df['Wettercode'].astype(str)\n",
    "\n",
    "# Liste der Kategorien für 'Wettercode'\n",
    "wettercode_categories = [str(i) for i in range(0, 100)] + ['Unbekannt']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        ('wettercode', OneHotEncoder(categories=[wettercode_categories], handle_unknown='ignore'), ['Wettercode']),\n",
    "        \n",
    "        ('warengruppe', OneHotEncoder(handle_unknown='ignore'), ['Warengruppe']),\n",
    "        \n",
    "        ('wochentag', OneHotEncoder(handle_unknown='ignore'), ['Wochentag'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Define your date thresholds\n",
    "train_end_date = '2017-07-31'\n",
    "validation_end_date = '2018-07-31'\n",
    "\n",
    "train_data = df[df['Datum'] <= train_end_date]\n",
    "test_data = df[df['Datum'] > train_end_date]\n",
    "\n",
    "df = df.drop('Datum', axis=1)\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "X = df.drop(['Umsatz'], axis=1)\n",
    "y = df['Umsatz']\n",
    "\n",
    "X_train  = train_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_train = train_data['Umsatz']\n",
    "\n",
    "X_test  = test_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_test = test_data['Umsatz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Transformation der Daten\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Überprüfen der Dimensionen der transformierten Daten\n",
    "input_shape = X_train_preprocessed.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "  InputLayer(shape=(input_shape, )),\n",
    "  BatchNormalization(),\n",
    "  Dense(72, activation='relu'),\n",
    "  Dropout(0.2),  # Regularisierung\n",
    "  Dense(36, activation='relu'),\n",
    "  Dropout(0.2),  # Regularisierung\n",
    "  Dense(18, activation='relu'),\n",
    "  Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train_preprocessed,\n",
    "    y_train,\n",
    "    validation_data=(X_test_preprocessed, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "print(\"R² on test:\", r2_score(y_test, y_pred))\n",
    "print(\"adjusted R² on test:\", adjr2(y_test, y_pred, len(y), X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten y_pred\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(y_test, y_pred_flat, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonale Linie\n",
    "plt.xlabel(\"Tatsächliche Werte\")\n",
    "plt.ylabel(\"Vorhergesagte Werte\")\n",
    "plt.title(\"Tatsächliche Werte vs. Vorhergesagte Werte\")\n",
    "plt.show()\n",
    "\n",
    "residuals = y_test - y_pred_flat\n",
    "\n",
    "# Residualplot\n",
    "plt.scatter(y_pred_flat, residuals, alpha=0.5, color='g')\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Vorhergesagte Werte\")\n",
    "plt.ylabel(\"Residuen\")\n",
    "plt.title(\"Residualplot\")\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Histogramm der Residuen\n",
    "sns.histplot(residuals, kde=True, color='y')\n",
    "plt.axvline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Residuen\")\n",
    "plt.title(\"Verteilung der Residuen\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y_test.values, label=\"Tatsächliche Werte\")\n",
    "plt.plot(y_pred_flat, label=\"Vorhergesagte Werte\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Umsatz\")\n",
    "plt.title(\"Vergleich der tatsächlichen und vorhergesagten Werte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell auf Submission-Datensatz laufen lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission vorbereiten\n",
    "\n",
    "\n",
    "df_sub = pd.read_csv('../sample_submission.csv')\n",
    "\n",
    "# Angenommen, die Jahre sind 20xx:\n",
    "df_sub['id'] = df_sub['id'].astype('string')\n",
    "df_sub['Tag'] = df_sub['id'].str[0:2]\n",
    "df_sub['Monat'] = df_sub['id'].str[2:4]\n",
    "df_sub['Jahr'] = '20' + df_sub['id'].str[4:6]  # falls im Format YY z. B. 01 → 2001 oder 20xx\n",
    "\n",
    "df_sub['Warengruppe'] = df_sub['id'].str[6]  # letztes Zeichen\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype(int)\n",
    "\n",
    "# Ein Datum aus Tag, Monat, Jahr erzeugen:\n",
    "df_sub.rename(columns={'Jahr':'year', 'Monat':'month', 'Tag':'day'}, inplace=True)\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub[['year','month','day']])\n",
    "df2['Datum'] = pd.to_datetime(df2['Datum'])\n",
    "df3['Datum'] = pd.to_datetime(df3['Datum'])\n",
    "df4['Datum'] = pd.to_datetime(df4['Datum'])\n",
    "df_sub = df_sub.merge(df4, on='Datum', how='left')\n",
    "df_sub = df_sub.merge(df3, on='Datum', how='left')\n",
    "df_sub = df_sub.merge(df2, on='Datum', how='left')\n",
    "\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5,6])\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat']/12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat']/12)\n",
    "\n",
    "# Check if all data types are correctly set for all variables\n",
    "print('### Initial datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "# Set the correct types for all variables\n",
    "\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].astype('category')\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype('category')\n",
    "\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].fillna(False)\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].astype('bool')\n",
    "print('### Corrected datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "\n",
    "# Datum in Datetime konvertieren\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub['Datum'])\n",
    "\n",
    "# Basismerkmale\n",
    "df_sub['Jahr'] = df_sub['Datum'].dt.year\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Kalenderwoche'] = df_sub['Datum'].dt.isocalendar().week\n",
    "df_sub['Tag_im_Jahr'] = df_sub['Datum'].dt.dayofyear\n",
    "df_sub['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "# Zyklische Merkmale\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Wochentag_sin'] = np.sin(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "df_sub['Wochentag_cos'] = np.cos(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "\n",
    "\n",
    "# Beispiel für Mittelwert-Imputation\n",
    "df_sub['Temperatur'].fillna(df_sub['Temperatur'].mean(), inplace=True)\n",
    "df_sub['Bewoelkung'].fillna(df_sub['Bewoelkung'].mean(), inplace=True)\n",
    "df_sub['Windgeschwindigkeit'].fillna(df_sub['Windgeschwindigkeit'].mean(), inplace=True)\n",
    "\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].cat.add_categories(['Unbekannt'])\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].fillna('Unbekannt')\n",
    "\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "#df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Vorhersagen treffen und evaluieren\n",
    "y_pred_sub = model.predict(df_sub)\n",
    "#y_pred_sub_best = grid_search.predict(df_sub)\n",
    "\n",
    "df_sub['Umsatz'] = y_pred_sub\n",
    "\n",
    "# Erstellen der finalen Submission:\n",
    "df_sub[['id','Umsatz']].to_csv('../nr_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE)\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): Actual values\n",
    "    y_pred (array-like): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "    float: MAPE value\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Ensure y_pred has the same number of elements as y_test\n",
    "y_pred = model.predict(X_test_preprocessed).flatten()\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"MAPE: {mape}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def r2(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" R2 Score \"\"\"\n",
    "    return r2_score(actual, predicted)\n",
    "\n",
    "def adjr2(actual: np.ndarray, predicted: np.ndarray, rowcount: int, featurecount: int):\n",
    "    \"\"\" Adjusted R2 Score \"\"\"\n",
    "    return 1-(1-r2(actual,predicted))*(rowcount-1)/(rowcount-featurecount)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE)\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): Actual values\n",
    "    y_pred (array-like): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "    float: MAPE value\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# 1. Daten einlesen und zusammenführen\n",
    "df1 = pd.read_csv('../umsatzdaten_gekuerzt.csv')\n",
    "df2 = pd.read_csv('../wetter.csv')\n",
    "df3 = pd.read_csv('../kiwo.csv')\n",
    "df4 = pd.read_csv('../Feier_Bruecke_Ferien_bis2018.csv')\n",
    "\n",
    "df = df1.merge(df2, on='Datum', how='left')\n",
    "df = df.merge(df3, on='Datum', how='left')\n",
    "df = df.merge(df4, on='Datum', how='left')\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df['KielerWoche'] = df['KielerWoche'].fillna(False)\n",
    "df['KielerWoche'] = df['KielerWoche'].astype('bool')\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# a) Feature Engineering: Erstellen von Datum- und zyklischen Features\n",
    "def add_features(df):\n",
    "    df['Jahr'] = df['Datum'].dt.year\n",
    "    df['Monat'] = df['Datum'].dt.month\n",
    "    df['Wochentag'] = df['Datum'].dt.weekday\n",
    "    df['Kalenderwoche'] = df['Datum'].dt.isocalendar().week\n",
    "    df['Tag_im_Jahr'] = df['Datum'].dt.dayofyear\n",
    "    df['Ist_Wochenende'] = df['Wochentag'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Zyklische Merkmale\n",
    "    df['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df['Tag_im_Jahr'] / 365)\n",
    "    df['Monat_sin'] = np.sin(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Monat_cos'] = np.cos(2 * np.pi * df['Monat'] / 12)\n",
    "    df['Wochentag_sin'] = np.sin(2 * np.pi * df['Wochentag'] / 7)\n",
    "    df['Wochentag_cos'] = np.cos(2 * np.pi * df['Wochentag'] / 7)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = add_features(df)\n",
    "# b) FunctionTransformer für Feature Engineering\n",
    "feature_engineering = FunctionTransformer(add_features, validate=False)\n",
    "\n",
    "# c) Definieren der Feature-Gruppen\n",
    "numeric_features = ['Temperatur', 'Bewoelkung', 'Windgeschwindigkeit',\n",
    "                    'Tag_im_Jahr_sin', 'Tag_im_Jahr_cos',\n",
    "                    'Monat_sin', 'Monat_cos',\n",
    "                    'Wochentag_sin', 'Wochentag_cos','feiertag', 'KielerWoche',\n",
    "                    'brueckentag', 'BW', 'BY', 'B', 'BB', 'HB', 'HH', 'HE', 'MV',\n",
    "                      'NI', 'NW', 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
    "                    \n",
    "categorical_features = ['Warengruppe', 'Wettercode', 'Wochentag']\n",
    "\n",
    "# d) Definieren des ColumnTransformers\n",
    "# Stellen Sie sicher, dass 'Wettercode' als String vorliegt\n",
    "df['Wettercode'] = df['Wettercode'].astype(str)\n",
    "\n",
    "# Liste der Kategorien für 'Wettercode'\n",
    "wettercode_categories = [str(i) for i in range(0, 100)] + ['Unbekannt']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        ('wettercode', OneHotEncoder(categories=[wettercode_categories], handle_unknown='ignore'), ['Wettercode']),\n",
    "        \n",
    "        ('warengruppe', OneHotEncoder(handle_unknown='ignore'), ['Warengruppe']),\n",
    "        \n",
    "        ('wochentag', OneHotEncoder(handle_unknown='ignore'), ['Wochentag'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Define your date thresholds\n",
    "train_end_date = '2017-07-31'\n",
    "validation_end_date = '2018-07-31'\n",
    "\n",
    "train_data = df[df['Datum'] <= train_end_date]\n",
    "test_data = df[df['Datum'] > train_end_date]\n",
    "\n",
    "df = df.drop('Datum', axis=1)\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "X = df.drop(['Umsatz'], axis=1)\n",
    "y = df['Umsatz']\n",
    "\n",
    "X_train  = train_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_train = train_data['Umsatz']\n",
    "upper_limit = y_train.quantile(0.995)\n",
    "y_train = np.clip(y_train, a_min=None, a_max=upper_limit)\n",
    "\n",
    "X_test  = test_data.drop(['Umsatz'], axis=1)  # Behalten Sie 'Datum' in X für die Pipeline\n",
    "y_test = test_data['Umsatz']\n",
    "\n",
    "# Transformation der Daten\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Überprüfen der Dimensionen der transformierten Daten\n",
    "input_shape = X_train_preprocessed.shape[1]\n",
    "\n",
    "# Modell\n",
    "model = Sequential([\n",
    "    InputLayer(shape=(input_shape, )),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='huber', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train_preprocessed,\n",
    "    y_train,\n",
    "    validation_data=(X_test_preprocessed, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "print(\"R² on test:\", r2_score(y_test, y_pred))\n",
    "print(\"adjusted R² on test:\", adjr2(y_test, y_pred, len(y), X.shape[1]))\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, y_pred)}%\")\n",
    "\n",
    "# Submission vorbereiten\n",
    "df_sub = pd.read_csv('../sample_submission.csv')\n",
    "\n",
    "# Angenommen, die Jahre sind 20xx:\n",
    "df_sub['id'] = df_sub['id'].astype('string')\n",
    "df_sub['Tag'] = df_sub['id'].str[4:6]\n",
    "df_sub['Monat'] = df_sub['id'].str[2:4]\n",
    "df_sub['Jahr'] = '20' + df_sub['id'].str[0:2]  # falls im Format YY z. B. 01 → 2001 oder 20xx\n",
    "\n",
    "df_sub['Warengruppe'] = df_sub['id'].str[6]  # letztes Zeichen\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype(int)\n",
    "\n",
    "# Ein Datum aus Tag, Monat, Jahr erzeugen:\n",
    "df_sub.rename(columns={'Jahr':'year', 'Monat':'month', 'Tag':'day'}, inplace=True)\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub[['year','month','day']])\n",
    "df2['Datum'] = pd.to_datetime(df2['Datum'])\n",
    "df3['Datum'] = pd.to_datetime(df3['Datum'])\n",
    "df4['Datum'] = pd.to_datetime(df4['Datum'])\n",
    "df_sub = df_sub.merge(df4, on='Datum', how='left')\n",
    "df_sub = df_sub.merge(df3, on='Datum', how='left')\n",
    "df_sub = df_sub.merge(df2, on='Datum', how='left')\n",
    "\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5,6])\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat']/12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat']/12)\n",
    "\n",
    "\n",
    "print(df_sub.head)\n",
    "# Check if all data types are correctly set for all variables\n",
    "print('### Initial datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "# Set the correct types for all variables\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].astype('category')\n",
    "df_sub['Warengruppe'] = df_sub['Warengruppe'].astype('category')\n",
    "\n",
    "# replace NaN with False for 'KielerWoche'\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].fillna(False)\n",
    "df_sub['KielerWoche'] = df_sub['KielerWoche'].astype('bool')\n",
    "print('### Corrected datatypes')\n",
    "print(df_sub.dtypes)\n",
    "\n",
    "# Datum in Datetime konvertieren\n",
    "df_sub['Datum'] = pd.to_datetime(df_sub['Datum'])\n",
    "\n",
    "# Basismerkmale\n",
    "df_sub['Jahr'] = df_sub['Datum'].dt.year\n",
    "df_sub['Monat'] = df_sub['Datum'].dt.month\n",
    "df_sub['Wochentag'] = df_sub['Datum'].dt.weekday\n",
    "df_sub['Kalenderwoche'] = df_sub['Datum'].dt.isocalendar().week\n",
    "df_sub['Tag_im_Jahr'] = df_sub['Datum'].dt.dayofyear\n",
    "df_sub['Tag_im_Jahr_sin'] = np.sin(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Tag_im_Jahr_cos'] = np.cos(2 * np.pi * df_sub['Tag_im_Jahr'] / 365)\n",
    "df_sub['Ist_Wochenende'] = df_sub['Wochentag'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Zyklische Merkmale\n",
    "df_sub['Monat_sin'] = np.sin(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Monat_cos'] = np.cos(2 * np.pi * df_sub['Monat'] / 12)\n",
    "df_sub['Wochentag_sin'] = np.sin(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "df_sub['Wochentag_cos'] = np.cos(2 * np.pi * df_sub['Wochentag'] / 7)\n",
    "\n",
    "# Beispiel für Mittelwert-Imputation\n",
    "df_sub['Temperatur'].fillna(df_sub['Temperatur'].mean(), inplace=True)\n",
    "df_sub['Bewoelkung'].fillna(df_sub['Bewoelkung'].mean(), inplace=True)\n",
    "df_sub['Windgeschwindigkeit'].fillna(df_sub['Windgeschwindigkeit'].mean(), inplace=True)\n",
    "\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].cat.add_categories(['Unbekannt'])\n",
    "df_sub['Wettercode'] = df_sub['Wettercode'].fillna('Unbekannt')\n",
    "\n",
    "# Sicherstellen, dass 'Datum' als datetime konvertiert ist\n",
    "#df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# Transform the submission data\n",
    "df_sub_preprocessed = preprocessor.transform(df_sub.drop(['id', 'Datum'], axis=1))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sub = model.predict(df_sub_preprocessed)\n",
    "\n",
    "df_sub['Umsatz'] = y_pred_sub\n",
    "\n",
    "# Erstellen der finalen Submission:\n",
    "df_sub[['id','Umsatz']].to_csv('../nr_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = pd.concat([X_train, y_train], axis=1).corr()\n",
    "print(correlation_matrix['Umsatz'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert datetime columns to numerical values\n",
    "X_train_preprocessed = X_train_preprocessed.astype(float)\n",
    "X_test_preprocessed = X_test_preprocessed.astype(float)\n",
    "\n",
    "def objective(trial):\n",
    "    history = model.fit(X_train_preprocessed, y_train, validation_data=(X_test_preprocessed, y_test), \n",
    "                        epochs=10, batch_size=trial.suggest_int('batch_size', 16, 128), verbose=0)\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Beste Parameter:\", study.best_params)\n",
    "print(\"Bestes Ergebnis (val_mae):\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Drop datetime columns if present\n",
    "X_train_rf = X_train.drop(columns=['Datum'], errors='ignore')\n",
    "X_test_rf = X_test.drop(columns=['Datum'], errors='ignore')\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train_rf, y_train)\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X_train_rf.columns)\n",
    "print(feature_importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiertes Modell\n",
    "\n",
    "\n",
    "selected_features = [\n",
    "    'Warengruppe', 'Temperatur', 'Tag_im_Jahr', 'Tag_im_Jahr_cos',\n",
    "    'id', 'Wochentag', 'Wochentag_sin', 'Ist_Wochenende', 'MV', 'BB'\n",
    "]\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train_selected.shape[1]),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "history = model.fit(\n",
    "    X_train_selected, y_train,\n",
    "    validation_data=(X_test_selected, y_test),\n",
    "    epochs=50, batch_size=51, verbose=1\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_selected, y_train)\n",
    "y_pred_rf = rf.predict(X_test_selected)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest MAE: {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(y_train)\n",
    "plt.title('Umsatz Boxplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
